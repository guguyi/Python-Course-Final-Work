{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import pandas\n",
    "import requests\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class html_parser(object):\n",
    "    weibo = []\n",
    "    cookies = {\n",
    "        'cookie': '_T_WM=94460060722; SCF=Akc6-rgt_orPQPcMl2x5HV85EqkMAXlAtNu4p7Xjq-Yk3pldnC21hCsqvkdyckOmI9VGTWz2-fh41X9Gv2M3pY0.; SUB=_2A25z2CfhDeRhGeBL7FES9i3LzzSIHXVRI0mprDV6PUJbkdANLUb3kW1NRssixUMbX_chzOWyuMU2qvh9yNV6fdq6; SUHB=0Ryq3AMeUc9U3v'}\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}\n",
    "\n",
    "    def __init__(self, filename,  txtname, csvname, wrote_num_txt, wrote_num_csv):\n",
    "        \"\"\"\n",
    "            filename: 爬取到的html页面（保存在txt文件中）\n",
    "            to_csvname:目标csv路径\n",
    "            to_txtname:目标txt路径\n",
    "            wrote_num_txt:txt已写入的记录个数\n",
    "            wrote_num_csv:csv已写入的记录个数\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        self.to_csvname = csvname\n",
    "        self.to_txtname = txtname\n",
    "        self.wrote_num_txt = wrote_num_txt\n",
    "        self.wrote_num_csv = wrote_num_csv\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"\n",
    "            生成lxml.etree._Element对象\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.filename, mode='r', encoding='utf-8') as f:\n",
    "                html = f.read()\n",
    "            # 删除<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "            string = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n",
    "            if string in html:\n",
    "                html = html.replace(string, '')\n",
    "            self.html = html\n",
    "            tree = etree.HTML(html)\n",
    "            self.tree = tree\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_tree error!\")\n",
    "\n",
    "    def deal_grabled(self, info):\n",
    "        \"\"\"\n",
    "            处理爬取原创微博时的乱码\n",
    "            info: lxml.etree._Element对象\n",
    "        \"\"\"\n",
    "        try:\n",
    "            info = (info.xpath('string(.)')).replace(u'\\u200b', '').encode(\n",
    "                sys.stdout.encoding, 'ignore').decode(sys.stdout.encoding)\n",
    "            return info\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"deal_grabled error!\")\n",
    "\n",
    "    def is_original(self, info):\n",
    "        \"\"\"\n",
    "            判断一条微博是否为原创微博\n",
    "            info: lxml.etree._Element对象\n",
    "        \"\"\"\n",
    "        is_original = info.xpath(\n",
    "            'div/span[@class=\"cmt\"]')  # 原创微博span标签中有class=\"cmt\"属性\n",
    "        if len(is_original) > 3:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def get_long_weibo(self, url):\n",
    "        \"\"\"\n",
    "            获取长原创微博\n",
    "            长原创微博需要再次请求url\n",
    "            url: 'https://weibo.cn/comment/'+weibo_id\n",
    "        \"\"\"\n",
    "        try:\n",
    "            r = requests.get(url, headers=self.headers, cookies=self.cookies)\n",
    "            r.raise_for_status()\n",
    "            r.encoding = r.apparent_encoding\n",
    "            html = r.content\n",
    "            tree = etree.HTML(html)\n",
    "            info = tree.xpath('//div[@class=\"c\"]')[1]  # 定位\n",
    "            content = self.deal_grabled(info)  # 处理乱码\n",
    "            time = info.xpath(\"//span[@class='ct']/text()\")[0]\n",
    "            weibo_content = content[content.find(':') +\n",
    "                                    1:content.rfind(time)]  # 分割\n",
    "            return weibo_content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_long_weibo error!\")\n",
    "\n",
    "    def get_original_weibo(self, info, weibo_id):\n",
    "        \"\"\"\n",
    "            获取原创微博\n",
    "            info: lxml.etree._Element对象\n",
    "            weibo_id: weibo对应的唯一id\n",
    "        \"\"\"\n",
    "        try:\n",
    "            weibo_content = self.deal_grabled(info)  # 处理乱码\n",
    "            weibo_content = weibo_content[:weibo_content.rfind(u'赞')]\n",
    "            a_text = info.xpath('div//a/text()')\n",
    "            if u'全文' in a_text:  # 长微博判断\n",
    "                weibo_link = 'https://weibo.cn/comment/' + weibo_id\n",
    "                content = self.get_long_weibo(weibo_link)\n",
    "                if content:\n",
    "                    weibo_content = content\n",
    "            return weibo_content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_original_weibo error!\")\n",
    "\n",
    "    def get_repost_weibo(self, info, weibo_id):\n",
    "        \"\"\"\n",
    "            获取转发微博的转发理由\n",
    "            info: lxml.etree._Element对象\n",
    "            weibo_id: weibo对应的唯一id\n",
    "        \"\"\"\n",
    "        try:\n",
    "            reason = self.deal_grabled(info.xpath('div')[-1])\n",
    "            reason = reason[:reason.rindex(u'赞')]\n",
    "            weibo_content = reason\n",
    "            return weibo_content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_repost_weibo error!\")\n",
    "\n",
    "    def get_weibo_content(self, info, is_original):\n",
    "        \"\"\"\n",
    "            获取微博正文\n",
    "            info: lxml.etree._Element对象\n",
    "            is_original: 是否为原创微博 对应原创微博的解析函数\n",
    "        \"\"\"\n",
    "        try:\n",
    "            weibo_id = info.xpath('@id')[0][2:]\n",
    "            if is_original:  # 原创微博处理\n",
    "                weibo_content = self.get_original_weibo(info, weibo_id)\n",
    "            else:  # 转发微博处理\n",
    "                weibo_content = self.get_repost_weibo(info, weibo_id)\n",
    "            print(weibo_content)  # 打印微博正文\n",
    "            return weibo_content\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_weibo_content error!\")\n",
    "\n",
    "    def get_one_weibo(self, info):\n",
    "        \"\"\"\n",
    "            获取某一条微博的相关信息\n",
    "        \"\"\"\n",
    "        try:\n",
    "            weibo = OrderedDict()\n",
    "            is_original = self.is_original(info)  # 是否为原创微博\n",
    "            weibo['id'] = info.xpath('@id')[0][2:]  # 微博id是第2位以后的字符串\n",
    "            weibo['content'] = self.get_weibo_content(\n",
    "                info, is_original)  # 获取微博内容\n",
    "            weibo['original'] = is_original  # 原创微博标识\n",
    "            return weibo\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_one_weibo error!\")\n",
    "\n",
    "    def get_all_weibo(self):\n",
    "        \"\"\"\n",
    "            获取一个html页面的全部微博\n",
    "        \"\"\"\n",
    "        try:\n",
    "            info = self.tree.xpath('//div[@class=\"c\"]')\n",
    "            is_exist = info[0].xpath('div/span[@class=\"ctt\"]')  # 判断当前页面是否有微博\n",
    "            if is_exist:\n",
    "                for i in range(len(info)-2):\n",
    "                    weibo = self.get_one_weibo(info[i])  # 获取一条微博的信息\n",
    "                    if weibo:\n",
    "                        self.weibo.append(weibo)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"get_all_weibo error!\")\n",
    "\n",
    "    def write_txt(self):\n",
    "        \"\"\"\n",
    "            将爬取到的微博写入txt文件中\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = []\n",
    "            self.new_txt_num = 0  # 新写入txt记录个数\n",
    "            for k, i in enumerate(self.weibo):\n",
    "                result.append('微博id: '+i['id']+'\\n'+u'微博正文/转发理由: '+i['content'] +\n",
    "                              '\\n'+'是否原创: '+str(i['original'])+'\\n\\n')\n",
    "                self.new_txt_num += 1  # 更新new_txt_num\n",
    "            self.wrote_num_txt += self.new_txt_num  # 更新wrote_num_txt\n",
    "            print(result)\n",
    "            self.result_txt = result\n",
    "            self.write_result = ''.join(result)\n",
    "            with open(self.to_txtname, 'a', encoding='utf-8') as f:\n",
    "                f.write(self.write_result)\n",
    "\n",
    "            print(\"write to .txt over!\")\n",
    "            print('write ' + str(self.new_txt_num) +\n",
    "                  ' weibo ' + 'to .txt file!')\n",
    "            print('There is already ' +\n",
    "                  str(self.wrote_num_txt)+' weibo in .txt file!')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"write_txt error!\")\n",
    "\n",
    "    def write_csv(self):\n",
    "        \"\"\"\n",
    "            将爬取到的微博写入csv文件中\n",
    "        \"\"\"\n",
    "        self.new_csv_num = 0  # 新写入csv记录个数\n",
    "        try:\n",
    "            result_headers = ['微博id', '微博正文/转发理由', '是否原创']\n",
    "            result_data = [w.values() for w in self.weibo]\n",
    "            # print(type(result_data))\n",
    "            # print(len(result_data))\n",
    "            self.new_csv_num = len(result_data)\n",
    "            print(result_data)\n",
    "\n",
    "            with open(self.to_csvname, 'a', encoding='utf-8-sig', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                if self.wrote_num_csv == 0:\n",
    "                    writer.writerows([result_headers])\n",
    "                writer.writerows(result_data)\n",
    "            self.wrote_num_csv += self.new_csv_num\n",
    "            print(\"write to .csv over!\")\n",
    "            print('write ' + str(self.new_csv_num) +\n",
    "                  ' weibo ' + 'to .csv file!')\n",
    "            print('There is already ' +\n",
    "                  str(self.wrote_num_csv)+' weibo in .csv file!')\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=html_parser('yyqx_txt/yyqx_2.txt','1.txt','2.csv',0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.get_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#奋斗吧青春# 《奋斗的青春最美丽——2020年五·四青年节特别节目》，今晚在CCTV1、CCTV3播出，一起奋斗一起追梦！  [组图共2张] 原图 \n",
      "转发理由:#五四致敬战疫青年# 致敬战疫青年，致敬最可爱的人！#谢谢你保护了我们#//@TFBOYS组合:#五四致敬战疫青年#五四青年节到来，让我们说一声，#谢谢你保护了我们#  \n",
      "#朋友请听好# 一起来听小站音乐会 朋友请听好第8期：千玺又被谢娜套路要跳拉丁？ 杨迪沉浸式读信扮演蟑螂笑Skr人  [组图共4张] 原图 \n",
      "今晚十点，湖南卫视#朋友请听好#，一起来听广播剧。  [组图共2张] 原图 \n",
      "#华为nova7#系列新品发布会对焦ing @华为终端官方微博 的一直播  [组图共2张] 原图 \n",
      "问题和烦恼，让书籍给你答案。#423听书节# 来@喜马拉雅  #有声图书馆#，寻找更多未知的答案。  原图 \n",
      "定义意式优雅，致敬传奇经典。#阿玛尼美妆20周年#，尽在5月14日，#天猫超级品牌日# 阿玛尼的微博视频  \n",
      "你好，#华为nova7#系列。nova7号电台等你来电 @华为终端官方微博 TFBOYS-易烊千玺的微博视频  \n",
      "转发理由:和学长在广播小屋相聚了，谢谢何老师  \n",
      "😆  \n"
     ]
    }
   ],
   "source": [
    "test.get_all_weibo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[odict_values(['J0oMQcPVn', '#奋斗吧青春# 《奋斗的青春最美丽——2020年五·四青年节特别节目》，今晚在CCTV1、CCTV3播出，一起奋斗一起追梦！ \\xa0[组图共2张]\\xa0原图\\xa0', True]), odict_values(['J0keOdHEi', '转发理由:#五四致敬战疫青年# 致敬战疫青年，致敬最可爱的人！#谢谢你保护了我们#//@TFBOYS组合:#五四致敬战疫青年#五四青年节到来，让我们说一声，#谢谢你保护了我们#\\xa0\\xa0', False]), odict_values(['IFyuniWZ0', '#朋友请听好# 一起来听小站音乐会 朋友请听好第8期：千玺又被谢娜套路要跳拉丁？ 杨迪沉浸式读信扮演蟑螂笑Skr人 \\xa0[组图共4张]\\xa0原图\\xa0', True]), odict_values(['IEH5zxf1L', '今晚十点，湖南卫视#朋友请听好#，一起来听广播剧。 \\xa0[组图共2张]\\xa0原图\\xa0', True]), odict_values(['IEGAgq2eU', '#华为nova7#系列新品发布会对焦ing @华为终端官方微博 的一直播 \\xa0[组图共2张]\\xa0原图\\xa0', True]), odict_values(['IEyyFiGxr', '问题和烦恼，让书籍给你答案。#423听书节# 来@喜马拉雅  #有声图书馆#，寻找更多未知的答案。 \\xa0原图\\xa0', True]), odict_values(['IDIgZ3EB7', '定义意式优雅，致敬传奇经典。#阿玛尼美妆20周年#，尽在5月14日，#天猫超级品牌日# 阿玛尼的微博视频 \\xa0', True]), odict_values(['IDAW6nFrE', '你好，#华为nova7#系列。nova7号电台等你来电 @华为终端官方微博 TFBOYS-易烊千玺的微博视频 \\xa0', True]), odict_values(['IDztOzhIe', '转发理由:和学长在广播小屋相聚了，谢谢何老师\\xa0\\xa0', False]), odict_values(['IDtcMcicK', '😆 \\xa0', True])]\n",
      "write to .csv over!\n",
      "write 10 weibo to .csv file!\n",
      "There is already 10 weibo in .csv file!\n"
     ]
    }
   ],
   "source": [
    "test.write_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
